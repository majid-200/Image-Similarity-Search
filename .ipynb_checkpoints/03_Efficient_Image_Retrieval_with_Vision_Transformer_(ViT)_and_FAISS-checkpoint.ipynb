{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd4edeb-f90f-4475-9bb4-b5e5b3171c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "from typing import List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "# Set up logging to track what the system is doing\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdcb196-5513-4419-8c32-f5c28ef4feb7",
   "metadata": {},
   "source": [
    "# Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56ce99e-9b42-4292-bbac-549540d1e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for batch processing of images.\n",
    "    This class wraps a list of image paths and makes them compatible with PyTorch's DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths: list, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: List of file paths to images\n",
    "            transform: Optional image transformations to apply\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return a single image at the given index.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the image to load\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (transformed_image, image_path)\n",
    "        \"\"\"\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            # Open the image and convert to RGB (in case it's grayscale or RGBA)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Apply transformations if provided (e.g., resizing, normalization)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, image_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b5f936-5dc4-4913-8c43-964d8c0580ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts visual features from images using a pre-trained Vision Transformer (ViT) model.\n",
    "    These features are numerical representations that capture the visual content of images.\n",
    "    \"\"\"\n",
    "    def __init__(self, device: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor with a ViT model.\n",
    "        \n",
    "        Args:\n",
    "            device: Device to run model on ('cuda' or 'cpu'). Auto-detects if None.\n",
    "        \"\"\"\n",
    "        # Automatically choose GPU if available, otherwise use CPU\n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load pre-trained Vision Transformer model (trained on ImageNet dataset)\n",
    "        self.model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Save the original forward method and replace it with our custom one\n",
    "        # This allows us to extract features instead of getting classification outputs\n",
    "        self.original_forward = self.model.forward\n",
    "        self.model.forward = self._forward_features\n",
    "        \n",
    "        # Set model to evaluation mode (disables dropout, batch norm training, etc.)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Move model to the appropriate device (GPU or CPU)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # ViT-B/16 outputs 768-dimensional feature vectors\n",
    "        self.feature_dim = 768\n",
    "        \n",
    "        # Define image preprocessing pipeline\n",
    "        # These transforms prepare images for the ViT model\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),  # Resize shorter side to 224 pixels\n",
    "            transforms.CenterCrop(224),  # Crop center 224x224 region\n",
    "            transforms.ToTensor(),  # Convert to PyTorch tensor (scales to [0,1])\n",
    "            # Normalize using ImageNet statistics (required for pre-trained models)\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        logger.info(f\"Initialized ViT feature extractor with dimension: {self.feature_dim}\")\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        \"\"\"\n",
    "        Modified forward pass to extract feature embeddings instead of class predictions.\n",
    "        \n",
    "        The Vision Transformer works by:\n",
    "        1. Splitting the image into patches\n",
    "        2. Processing patches through transformer layers\n",
    "        3. Using a special [CLS] token to represent the entire image\n",
    "        \n",
    "        We return the [CLS] token embedding as our feature vector.\n",
    "        \n",
    "        Args:\n",
    "            x: Input image tensor\n",
    "            \n",
    "        Returns:\n",
    "            Feature vector (embedding) for the image\n",
    "        \"\"\"\n",
    "        # Process input through the model's initial layers\n",
    "        x = self.model._process_input(x)\n",
    "        n = x.shape[0]  # Batch size\n",
    "\n",
    "        # Add the class token to the beginning of the sequence\n",
    "        # This token will aggregate information about the entire image\n",
    "        cls_token = self.model.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        \n",
    "        # Pass through transformer encoder layers\n",
    "        x = self.model.encoder(x)\n",
    "\n",
    "        # Return only the CLS token embedding (first token)\n",
    "        # This is a 768-dimensional vector representing the entire image\n",
    "        return x[:, 0]\n",
    "\n",
    "    @torch.no_grad()  # Disable gradient computation for faster inference\n",
    "    def extract_features(self, image_path: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract a feature vector from a single image.\n",
    "        \n",
    "        This is the main method to use - give it an image path and get back\n",
    "        a numerical vector that represents that image's visual content.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "            \n",
    "        Returns:\n",
    "            768-dimensional normalized feature vector as numpy array\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load and transform the image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Extract features using the model\n",
    "            features = self.model(image)\n",
    "            \n",
    "            # Convert from PyTorch tensor to numpy array and remove batch dimension\n",
    "            features = features.cpu().numpy().squeeze()\n",
    "            \n",
    "            # Verify we got the expected feature dimension\n",
    "            if features.shape != (self.feature_dim,):\n",
    "                raise ValueError(f\"Unexpected feature dimension: {features.shape}\")\n",
    "            \n",
    "            # L2 normalization: make the feature vector have unit length\n",
    "            # This makes distance comparisons more meaningful\n",
    "            norm = np.linalg.norm(features)\n",
    "            if norm > 0:\n",
    "                features = features / norm\n",
    "            \n",
    "            logger.debug(f\"Extracted features shape: {features.shape}\")\n",
    "            logger.debug(f\"Features norm: {np.linalg.norm(features)}\")\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting features from {image_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"\n",
    "        Cleanup method called when the object is destroyed.\n",
    "        Restores the original forward method to avoid side effects.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'original_forward'):\n",
    "            self.model.forward = self.original_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1f629-58b4-44bc-b77e-df2ee12b8a80",
   "metadata": {},
   "source": [
    "# Retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd637bb-2c20-4662-8528-45e3ea5a2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Retrieval System\n",
    "\n",
    "# This system finds similar images using AI-powered visual search. Think of it like\n",
    "# a reverse image search, but for your own photo collection.\n",
    "\n",
    "# How it works:\n",
    "\n",
    "# 1. Feature Extraction: \n",
    "#    - Converts each image into a \"fingerprint\" (768 numbers that describe what's in the image)\n",
    "#    - Similar images have similar fingerprints\n",
    "\n",
    "# 2. Indexing: \n",
    "#    - Processes all your images and stores their fingerprints in a searchable database\n",
    "#    - Uses FAISS (Facebook AI Similarity Search) for efficient searching\n",
    "#    - Saves metadata about each image (filename, path, when it was indexed)\n",
    "\n",
    "# 3. Search: \n",
    "#    - Takes a query image and finds images with similar fingerprints\n",
    "#    - Returns the most similar images ranked by similarity\n",
    "#    - Uses IndexIVFFlat for fast searching in large collections\n",
    "\n",
    "# About IndexIVFFlat (Inverted File with Flat quantizer):\n",
    "#     - Organizes images into clusters/regions (like organizing books by genre)\n",
    "#     - When searching:\n",
    "#       * First finds which clusters are most relevant to your query\n",
    "#       * Only searches within those clusters (much faster!)\n",
    "#     - Two-step setup:\n",
    "#       * Training: Learns how to group images into clusters\n",
    "#       * nprobe: How many clusters to search (more = slower but more accurate)\n",
    "#     - Trade-off: Speed vs accuracy (might miss some matches, but usually finds good ones)\n",
    "#     - Best for large image collections (1000+ images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac203bc-dd17-4ea3-ab46-15626caff7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRetrievalSystem:\n",
    "    \"\"\"\n",
    "    Main system for indexing and searching images based on visual similarity.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 feature_extractor: Optional[ImageFeatureExtractor] = None,\n",
    "                 index_path: Optional[str] = None,\n",
    "                 metadata_path: Optional[str] = None,\n",
    "                 use_gpu: bool = False,\n",
    "                 n_regions: int = 100,  # Number of clusters to divide images into\n",
    "                 nprobe: int = 10):     # Number of clusters to search\n",
    "        \"\"\"\n",
    "        Initialize the image retrieval system.\n",
    "        \n",
    "        Args:\n",
    "            feature_extractor: Object to extract features from images\n",
    "            index_path: Path to load existing FAISS index from\n",
    "            metadata_path: Path to load existing metadata from\n",
    "            use_gpu: Whether to use GPU acceleration for FAISS\n",
    "            n_regions: Number of clusters/regions for IVF index\n",
    "            nprobe: Number of regions to search (higher = more accurate but slower)\n",
    "        \"\"\"\n",
    "        # Use provided feature extractor or create a new one\n",
    "        self.feature_extractor = feature_extractor or ImageFeatureExtractor()\n",
    "        self.feature_dim = self.feature_extractor.feature_dim\n",
    "        self.n_regions = n_regions\n",
    "        self.nprobe = nprobe\n",
    "        logger.info(f\"Initializing retrieval system with dimension: {self.feature_dim}\")\n",
    "        \n",
    "        # Dictionary to store information about each indexed image\n",
    "        self.metadata = {}\n",
    "        \n",
    "        # Flag to track if the index has been trained\n",
    "        self.is_trained = False\n",
    "        \n",
    "        # Load existing index if paths are provided\n",
    "        if index_path and metadata_path:\n",
    "            self.load(index_path, metadata_path)\n",
    "        else:\n",
    "            # Create a new FAISS index from scratch\n",
    "            logger.info(f\"Creating new IVF index with {n_regions} regions\")\n",
    "            \n",
    "            # Quantizer: measures distances between feature vectors\n",
    "            self.quantizer = faiss.IndexFlatL2(self.feature_dim)\n",
    "            \n",
    "            # Main index: uses IVF (Inverted File) for faster search\n",
    "            # METRIC_L2 means we use Euclidean distance to measure similarity\n",
    "            self.index = faiss.IndexIVFFlat(self.quantizer, self.feature_dim, \n",
    "                                          self.n_regions, faiss.METRIC_L2)\n",
    "            \n",
    "            # Set how many regions to search during queries\n",
    "            self.index.nprobe = self.nprobe\n",
    "            \n",
    "            # Optionally move index to GPU for faster processing\n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    res = faiss.StandardGpuResources()\n",
    "                    self.index = faiss.index_cpu_to_gpu(res, 0, self.index)\n",
    "                    logger.info(\"Successfully moved index to GPU\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to use GPU, falling back to CPU: {str(e)}\")\n",
    "\n",
    "    def index_images(self, \n",
    "                    image_dir: str, \n",
    "                    batch_size: int = 32,\n",
    "                    num_workers: int = 4) -> None:\n",
    "        \"\"\"\n",
    "        Process and index all images in a directory.\n",
    "        \n",
    "        This creates the searchable database of image features.\n",
    "        \n",
    "        Args:\n",
    "            image_dir: Directory containing images to index\n",
    "            batch_size: Number of images to process at once (unused in current implementation)\n",
    "            num_workers: Number of parallel workers (unused in current implementation)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Indexing images from {image_dir}\")\n",
    "        \n",
    "        # Find all image files in the directory\n",
    "        image_paths = [\n",
    "            os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp'))\n",
    "        ]\n",
    "        \n",
    "        # Lists to store features and their corresponding paths\n",
    "        features_list = []\n",
    "        valid_paths = []\n",
    "        \n",
    "        # Process each image one by one\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                # Extract feature vector from the image\n",
    "                features = self.feature_extractor.extract_features(img_path)\n",
    "                features_list.append(features)\n",
    "                valid_paths.append(img_path)\n",
    "                logger.info(f\"Processed {img_path}\")\n",
    "            except Exception as e:\n",
    "                # Skip images that can't be processed\n",
    "                logger.error(f\"Error processing {img_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Make sure we successfully processed at least some images\n",
    "        if not features_list:\n",
    "            raise ValueError(\"No valid features extracted from images\")\n",
    "            \n",
    "        # Stack all feature vectors into a single numpy array\n",
    "        # Shape will be (num_images, 768)\n",
    "        all_features = np.stack(features_list)\n",
    "        logger.info(f\"Feature array shape: {all_features.shape}\")\n",
    "        logger.info(f\"Feature stats - Min: {all_features.min():.4f}, Max: {all_features.max():.4f}\")\n",
    "        \n",
    "        # Train the IVF index to learn how to cluster the features\n",
    "        # This only needs to be done once when first creating the index\n",
    "        if not self.is_trained:\n",
    "            logger.info(\"Training IVF index...\")\n",
    "            self.index.train(all_features)\n",
    "            self.is_trained = True\n",
    "            logger.info(\"Index training completed\")\n",
    "        \n",
    "        # Add all feature vectors to the searchable index\n",
    "        self.index.add(all_features)\n",
    "        logger.info(f\"Total vectors in index: {self.index.ntotal}\")\n",
    "        \n",
    "        # Store metadata for each indexed image\n",
    "        # This allows us to retrieve the original image path later\n",
    "        for idx, path in enumerate(valid_paths):\n",
    "            self.metadata[str(idx)] = {\n",
    "                'path': path,\n",
    "                'filename': os.path.basename(path),\n",
    "                'indexed_at': datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        logger.info(f\"Successfully indexed {len(valid_paths)} images\")\n",
    "\n",
    "    def search(self, \n",
    "              query_image_path: str,\n",
    "              k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Search for images similar to a query image.\n",
    "        \n",
    "        Args:\n",
    "            query_image_path: Path to the image to search for\n",
    "            k: Number of similar images to return\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples: (image_path, distance_score)\n",
    "            Lower distance = more similar\n",
    "        \"\"\"\n",
    "        logger.info(f\"Searching for similar images to {query_image_path}\")\n",
    "        logger.info(f\"Total images in index: {self.index.ntotal}\")\n",
    "        logger.info(f\"Available metadata keys: {list(self.metadata.keys())}\")\n",
    "        \n",
    "        # Make sure the index has been trained before searching\n",
    "        if not self.is_trained:\n",
    "            raise RuntimeError(\"Index has not been trained. Add images first.\")\n",
    "        \n",
    "        # Extract features from the query image\n",
    "        query_features = self.feature_extractor.extract_features(query_image_path)\n",
    "        logger.info(f\"Query feature shape: {query_features.shape}\")\n",
    "        \n",
    "        # Don't try to return more results than we have images\n",
    "        k = min(k, self.index.ntotal)\n",
    "        \n",
    "        # Search the index for similar feature vectors\n",
    "        # Returns: distances (how different) and indices (which images)\n",
    "        distances, indices = self.index.search(\n",
    "            query_features.reshape(1, -1),  # Reshape to (1, 768) for batch format\n",
    "            k\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Raw search results - distances: {distances[0]}\")\n",
    "        logger.info(f\"Raw search results - indices: {indices[0]}\")\n",
    "        logger.info(f\"Searched {self.nprobe} out of {self.n_regions} regions\")\n",
    "        \n",
    "        # Convert indices to image paths using metadata\n",
    "        results = []\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            str_idx = str(int(idx))\n",
    "            if str_idx in self.metadata:\n",
    "                # Store the image path and its distance score\n",
    "                results.append((self.metadata[str_idx]['path'], float(dist)))\n",
    "                logger.info(f\"Match found: {self.metadata[str_idx]['path']} with distance {dist:.3f}\")\n",
    "            else:\n",
    "                logger.warning(f\"Index {idx} not found in metadata\")\n",
    "        \n",
    "        # Sort by distance (smaller distance = more similar)\n",
    "        results.sort(key=lambda x: x[1])\n",
    "        \n",
    "        if not results:\n",
    "            logger.warning(\"No matches found!\")\n",
    "        else:\n",
    "            logger.info(f\"Found {len(results)} matches\")\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def save(self, index_path: str, metadata_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the index and metadata to disk for later use.\n",
    "        \n",
    "        This allows you to index once and search many times without re-indexing.\n",
    "        \n",
    "        Args:\n",
    "            index_path: Where to save the FAISS index\n",
    "            metadata_path: Where to save the metadata JSON\n",
    "        \"\"\"\n",
    "        # If using GPU, convert back to CPU for saving\n",
    "        if faiss.get_num_gpus() > 0:\n",
    "            self.index = faiss.index_gpu_to_cpu(self.index)\n",
    "            \n",
    "        # Save FAISS index (the searchable database)\n",
    "        faiss.write_index(self.index, index_path)\n",
    "        \n",
    "        # Save metadata (image paths and info) as JSON\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(self.metadata, f)\n",
    "            \n",
    "        logger.info(f\"Saved index with {self.index.ntotal} vectors\")\n",
    "        logger.info(f\"Saved index to {index_path} and metadata to {metadata_path}\")\n",
    "\n",
    "    def load(self, index_path: str, metadata_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load a previously saved index and metadata from disk.\n",
    "        \n",
    "        Args:\n",
    "            index_path: Path to the saved FAISS index\n",
    "            metadata_path: Path to the saved metadata JSON\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading index from {index_path}\")\n",
    "        \n",
    "        # Load the FAISS index\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        \n",
    "        # Loaded indexes are already trained\n",
    "        self.is_trained = True\n",
    "        \n",
    "        # Set nprobe for the loaded index (how many regions to search)\n",
    "        if isinstance(self.index, faiss.IndexIVFFlat):\n",
    "            self.index.nprobe = self.nprobe\n",
    "            logger.info(f\"Set nprobe to {self.nprobe} for loaded IVF index\")\n",
    "        \n",
    "        # Load the metadata\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "            \n",
    "        logger.info(f\"Loaded index with {self.index.ntotal} vectors\")\n",
    "        logger.info(f\"Metadata contains {len(self.metadata)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a050f1e-1c8f-4ffa-9885-1b435d887b08",
   "metadata": {},
   "source": [
    "# Index and retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5822553-4d69-4d29-87af-a201927da96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Application Runner\n",
    "\n",
    "# This section provides a simple interface to use the image retrieval system.\n",
    "# It handles two main tasks:\n",
    "\n",
    "# 1. INDEXING (One-time setup):\n",
    "#    - Processes all your images\n",
    "#    - Creates a searchable database\n",
    "#    - Saves it for future use\n",
    "   \n",
    "# 2. SEARCHING (Repeated use):\n",
    "#    - Loads the saved database\n",
    "#    - Finds images similar to your query\n",
    "#    - Shows results with similarity scores\n",
    "\n",
    "# Key Features:\n",
    "# - Automatic optimization based on your collection size\n",
    "# - Smart error handling\n",
    "# - Flexible configuration options\n",
    "# - User-friendly result display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81db41bc-3675-41f0-aca2-204aaef054d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for OpenMP warning on some systems\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77e2f30-c70d-463b-9e50-53fb2230bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_regions(num_images: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the optimal number of clusters/regions for the IVF index.\n",
    "    \n",
    "    This is important for performance:\n",
    "    - Too few regions: slow searches\n",
    "    - Too many regions: inefficient clustering\n",
    "    \n",
    "    Rule of thumb:\n",
    "    - Small collections (<100 images): sqrt(N) regions\n",
    "    - Larger collections: 4*sqrt(N) regions\n",
    "    \n",
    "    Args:\n",
    "        num_images: Total number of images to index\n",
    "        \n",
    "    Returns:\n",
    "        Optimal number of regions\n",
    "    \"\"\"\n",
    "    if num_images < 100:\n",
    "        # For small datasets, use fewer regions\n",
    "        n_regions = max(1, int(math.sqrt(num_images)))\n",
    "    else:\n",
    "        # For larger datasets, use more regions but cap at half the dataset size\n",
    "        n_regions = min(int(4 * math.sqrt(num_images)), num_images // 2)\n",
    "    return n_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "385bb002-e2d3-4c14-a46c-cfe348788cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    \"\"\"\n",
    "    Display search results in a user-friendly format.\n",
    "    \n",
    "    Shows each result with:\n",
    "    - Rank number\n",
    "    - Filename\n",
    "    - Full path\n",
    "    - Similarity score (higher = more similar)\n",
    "    - Distance score (lower = more similar)\n",
    "    \n",
    "    Args:\n",
    "        results: List of (image_path, distance) tuples\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"\\nNo matches found!\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\nSearch Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (path, distance) in enumerate(results, 1):\n",
    "        # Convert distance to similarity (0-1 scale, higher is better)\n",
    "        similarity = 1.0 / (1.0 + distance)\n",
    "        filename = os.path.basename(path)\n",
    "        \n",
    "        print(f\"{i}. Image: {filename}\")\n",
    "        print(f\"   Full path: {path}\")\n",
    "        print(f\"   Similarity Score: {similarity:.3f}\")\n",
    "        print(f\"   Distance: {distance:.3f}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb715fa6-e22b-4e82-bfdf-9e4f53361863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_image_retrieval(\n",
    "    task: str = \"index\",              # \"index\" or \"search\"\n",
    "    image_dir: str = None,            # Directory of images to index\n",
    "    query_image: str = None,          # Image to search for\n",
    "    index_path: str = \"image_index.faiss\",\n",
    "    metadata_path: str = \"image_metadata.json\",\n",
    "    num_results: int = 5,             # How many similar images to return\n",
    "    n_regions: int = None,            # Number of clusters (auto-calculated if None)\n",
    "    nprobe: int = None,               # Number of clusters to search (auto-calculated if None)\n",
    "    use_gpu: bool = False             # Use GPU acceleration\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the image retrieval system.\n",
    "    \n",
    "    This is the primary interface for both indexing and searching.\n",
    "    \n",
    "    INDEXING MODE (task=\"index\"):\n",
    "        - Processes all images in image_dir\n",
    "        - Creates and saves a searchable index\n",
    "        - Automatically optimizes settings based on collection size\n",
    "        \n",
    "    SEARCH MODE (task=\"search\"):\n",
    "        - Loads existing index\n",
    "        - Finds similar images to query_image\n",
    "        - Displays results with similarity scores\n",
    "    \n",
    "    Args:\n",
    "        task: Either \"index\" or \"search\"\n",
    "        image_dir: Directory containing images to index (required for indexing)\n",
    "        query_image: Path to query image (required for searching)\n",
    "        index_path: Where to save/load the FAISS index\n",
    "        metadata_path: Where to save/load the metadata\n",
    "        num_results: Number of similar images to return\n",
    "        n_regions: Number of clusters for IVF (auto-calculated if None)\n",
    "        nprobe: Number of clusters to search (auto-calculated if None)\n",
    "        use_gpu: Whether to use GPU acceleration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if task.lower() == 'index':\n",
    "            # ===== INDEXING MODE =====\n",
    "            \n",
    "            if not image_dir:\n",
    "                raise ValueError(\"image_dir is required for indexing task\")\n",
    "            \n",
    "            # Count how many images we have\n",
    "            image_files = [f for f in os.listdir(image_dir) \n",
    "                         if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp'))]\n",
    "            num_images = len(image_files)\n",
    "            \n",
    "            # Calculate optimal settings if not provided\n",
    "            if n_regions is None:\n",
    "                n_regions = calculate_optimal_regions(num_images)\n",
    "            if nprobe is None:\n",
    "                # Search about 1/4 of the regions by default\n",
    "                nprobe = max(1, n_regions // 4)\n",
    "                \n",
    "            logger.info(f\"Number of images: {num_images}\")\n",
    "            logger.info(f\"Using {n_regions} regions and searching {nprobe} regions\")\n",
    "                \n",
    "            # Create the retrieval system\n",
    "            retrieval_system = ImageRetrievalSystem(\n",
    "                n_regions=n_regions,\n",
    "                nprobe=nprobe,\n",
    "                use_gpu=use_gpu\n",
    "            )\n",
    "            \n",
    "            # Process all images and build the index\n",
    "            logger.info(f\"Indexing images from {image_dir}\")\n",
    "            retrieval_system.index_images(image_dir=image_dir)\n",
    "            \n",
    "            # Save for later use\n",
    "            logger.info(\"Saving index and metadata\")\n",
    "            retrieval_system.save(index_path, metadata_path)\n",
    "            \n",
    "        elif task.lower() == 'search':\n",
    "            # ===== SEARCH MODE =====\n",
    "            \n",
    "            if not query_image:\n",
    "                raise ValueError(\"query_image is required for search task\")\n",
    "            \n",
    "            # Make sure the index exists\n",
    "            if not os.path.exists(index_path) or not os.path.exists(metadata_path):\n",
    "                raise ValueError(f\"Index or metadata file not found. Please ensure both exist:\\n\"\n",
    "                               f\"Index: {index_path}\\nMetadata: {metadata_path}\")\n",
    "                \n",
    "            # Load the existing index\n",
    "            logger.info(f\"Loading existing index for search\")\n",
    "            retrieval_system = ImageRetrievalSystem(\n",
    "                index_path=index_path,\n",
    "                metadata_path=metadata_path,\n",
    "                nprobe=nprobe if nprobe is not None else 10,\n",
    "                use_gpu=use_gpu\n",
    "            )\n",
    "            \n",
    "            # Search for similar images\n",
    "            logger.info(f\"Searching for similar images to {query_image}\")\n",
    "            results = retrieval_system.search(\n",
    "                query_image_path=query_image,\n",
    "                k=num_results\n",
    "            )\n",
    "            \n",
    "            # Display the results\n",
    "            print_results(results)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Task must be either 'index' or 'search'\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f596e-e541-47d8-a360-64f99e66df6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CV)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
